{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "######################################################### Change Input Below ################################################\n",
    "filename = r\"C:\\Users\\n1603499\\OneDrive - Liberty Mutual\\Documents\\006.01  Data Science Challenge(DSC 2022)\\sprint_real_dataset.parquet\"\n",
    "\n",
    "my_epoc = 100\n",
    "feature_cat_name = [\"Loss_Type\", \"Damage_Type\",\"Lot_Run_Condition\",\"Lot_Make\"]\n",
    "y_name = \"Sale_Price\"\n",
    "#Adjust Learning rate\n",
    "myOptimizer = Adam(learning_rate = 0.1)\n",
    "# Activation functions to try out\n",
    "activations = ['relu','tanh','sigmoid']\n",
    "seed = 10\n",
    "my_funct = 'relu'\n",
    "n_layer = 4\n",
    "n_node = 20\n",
    "loss = 'mse'\n",
    "optimizer = 'adam'\n",
    "n_data = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################### Change Input Above ################################################\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ Define the model @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "def make_NN(X,y):\n",
    "    global scaler_x, scaler_y\n",
    "    scaler_x = MinMaxScaler( feature_range=(0,1))\n",
    "    scaler_y = MinMaxScaler( feature_range=(0,1))\n",
    "\n",
    "    scaled_X = scaler_x.fit_transform(X)\n",
    "    y = pd.DataFrame(y)\n",
    "    scaled_y = scaler_y.fit_transform(y)\n",
    "\n",
    "    scaler_x.fit(X)\n",
    "    scaler_y.fit(y)\n",
    "\n",
    "    m = scaler_y.scale_[0]\n",
    "    k = scaler_y.min_[0]\n",
    "    print(\"Note: y values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(m, k))\n",
    "    input_nCol = len(X.axes[1])\n",
    "    n_data = len(X.axes[0])\n",
    "    model = MakeModelDL(input_shape = input_nCol,n_layers= n_layer,n_nodes=n_node,act_function= my_funct,task_type=\"regressionPlus\")\n",
    "    model.compile(loss= loss,optimizer=optimizer)\n",
    "    model.fit(scaled_X,scaled_y,epochs=my_epoc,shuffle=True,verbose =0)\n",
    "    return model\n",
    "def NN_Predict(model,x_new):\n",
    "    scaled_x = scaler_x.transform(x_new)\n",
    "    predict_val = model.predict(scaled_x)\n",
    "    predict_val = scaler_y.inverse_transform(predict_val)\n",
    "    return predict_val\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"H:\\W_Documents\\DSC 2022\\sprint_real_dataset.parquet\"\n",
    "parquet_file = pd.read_parquet(filepath)\n",
    "# replace 'na' and 'nan' with np.nan\n",
    "parquet_file = parquet_file.replace('na', np.nan)\n",
    "parquet_file = parquet_file.replace('nan', np.nan)\n",
    "# parquet_file.info(verbose = True, null_counts=True)\n",
    "# remove date data out of numeric df\n",
    "date_columns = ['Auction_DT_week', 'Auction_DT_month','Auction_DT_year']\n",
    "not_use_col = ['Auction_DT_week', 'Auction_DT_month','Auction_DT_year','ElectricIndicator_HLDI','odomread','acv_pred_price',\"RedesignYear\",'Odometer_reading', \"additional_collision_points_desc\" , \"claim_open_dt\" , \"claim_rptd_dt\" , \"CVRG_TYPE_CD\" , \"external_source_type\" , \"init_point_of_impct_type\" , \"loss_st_cd\" , \"policy_st_cd\" , \"text_description\" , \"vehicl_color_nme\" , \"vehicl_towed_type\" , \"Copart_Facility_Name\" , \"FNOL_DT\" , \"Keys\" , \"Loss_DT\" , \"Pickup_DT\" , \"Sale_Title_Type\" , \"BasePrice_DDM4_Desc\" , \"BodyStyle\"]\n",
    "parquet_file = parquet_file.drop(not_use_col,axis=1)\n",
    "parquet_file = parquet_file[parquet_file['Lot_ACV'] != 0]\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "# import data and set lot id as index (use id to join later if we did seperate)\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "## checking for data types - can check this too with info() method\n",
    "dtype=parquet_file.dtypes\n",
    "cate_var = []\n",
    "num_var=[]\n",
    "ot_var=[]\n",
    "mySeed = 12\n",
    "n = 20\n",
    "for f, t in dtype.items():\n",
    "    if t==object:\n",
    "        cate_var.append(f)\n",
    "    elif t==float:\n",
    "        num_var.append(f)\n",
    "    else:\n",
    "        ot_var.append(f)     \n",
    "cate_used_var=[\"Damage_Type\",\"Loss_Type\",\"Lot_Make\",\"Lot_Model\"]\n",
    "data_cate=parquet_file[cate_used_var]\n",
    "data_num=parquet_file[num_var]\n",
    "fulldata=pd.concat([data_cate,data_num],axis=1)\n",
    "\n",
    "fulldata = fulldata[0:n_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "def create_label_encoding_with_min_count(fulldata, column, min_count=0.01*fulldata.shape[0]):\n",
    "    column_counts = fulldata.groupby([column])[column].transform(\"count\")\n",
    "    column_values = np.where(column_counts >= min_count, fulldata[column], \"\")\n",
    "    fulldata[column+\"_label\"] = preprocessing.LabelEncoder().fit_transform(column_values)\n",
    "    \n",
    "    return fulldata[column+\"_label\"]\n",
    "for cat in cate_used_var:\n",
    "    fulldata[cat]=create_label_encoding_with_min_count(fulldata, cat)\n",
    "    \n",
    "#X = data.drop(['Sale_Price','Loss_Type','Damage_Type', 'Lot_Run_Condition', 'Lot_Make'],axis=1)\n",
    "#Y = data['Sale_Price']\n",
    "X = fulldata.drop(['Sale_Price','Lot_ACV','Damage_Type', 'Loss_Type', 'Lot_Make', 'Lot_Model'],axis=1)\n",
    "\n",
    "parquet_file = pd.get_dummies(parquet_file, columns =feature_cat_name )\n",
    "parquet_file = parquet_file[parquet_file['Lot_ACV'] != 0]\n",
    "for f, t in dtype.items():\n",
    "    if t==object:\n",
    "        cate_var.append(f)\n",
    "    elif t==float:\n",
    "        num_var.append(f)\n",
    "    else:\n",
    "        ot_var.append(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test = train_test_split(X,train_size=0.7,random_state=mySeed)\n",
    "y_train, y_test = train_test_split(fulldata['Sale_Price'],train_size=0.7,random_state=mySeed)\n",
    "\n",
    "sale_price_train, sale_price_test = train_test_split(fulldata['Sale_Price'],train_size=0.7,random_state=mySeed)\n",
    "sale_price_train.reset_index(drop = True,inplace = True)\n",
    "sale_price_test.reset_index(drop = True,inplace = True)\n",
    "lot_ACV_train, lot_ACV_test= train_test_split(fulldata['Lot_ACV'],train_size=0.7,random_state=mySeed)\n",
    "lot_ACV_train.reset_index(drop = True,inplace = True)\n",
    "lot_ACV_test.reset_index(drop = True,inplace = True)\n",
    "\n",
    "y_train = sale_price_train/lot_ACV_train\n",
    "lot_ACV_train = np.array(lot_ACV_train)\n",
    "lot_ACV_test = np.array(lot_ACV_test)\n",
    "\n",
    "\n",
    "\n",
    "scale_train = MinMaxScaler( feature_range=(0,1))\n",
    "scale_valid = MinMaxScaler( feature_range=(0,1))\n",
    "scale_holdout = MinMaxScaler( feature_range=(0,1))\n",
    "\n",
    "# scale_y = MinMaxScaler(feature_range=(0,1))\n",
    "# X_train = train_df.loc[:,feature_cat_name]\n",
    "# X_valid = valid_df.loc[:,feature_cat_name]\n",
    "# X_holdout = holdout_df.loc[:,feature_cat_name]\n",
    "# X_train =  train_df.drop([y_name],axis=1)\n",
    "# X_valid = valid_df.drop([y_name],axis=1)\n",
    "# X_holdout = holdout_df.drop([y_name],axis=1)\n",
    "\n",
    "# y_train = train_df[[y_name]]\n",
    "# y_valid = valid_df[[y_name]]\n",
    "# y_holdout = holdout_df[[y_name]]\n",
    "\n",
    "y_valid = y_test\n",
    "X_valid = X_test\n",
    "\n",
    "# scaled_X_train = scale_train.fit_transform(X_train)\n",
    "# scaled_X_valid = scale_valid.fit_transform(X_valid)\n",
    "# # scaled_X_holdout = scale_train.fit_transform(X_holdout)\n",
    "\n",
    "# # scaled_y_holdout = scale_train.fit_transform(y_holdout)\n",
    "# y_valid = pd.DataFrame(y_valid)\n",
    "# y_train = pd.DataFrame(y_train)\n",
    "# scaled_y_valid = scale_valid.fit_transform(y_valid)\n",
    "# scaled_y_train = scale_train.fit_transform(y_train)\n",
    "\n",
    "# print(scaled_X_train[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeModelDL(input_shape,n_layers,n_nodes,act_function = 'relu',task_type = 'regression'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_nodes, input_shape = (input_shape,),activation= act_function ) )\n",
    "    for i in range(n_layers-1):\n",
    "        model.add(Dense(n_nodes, input_shape = (input_shape,),activation= act_function ))\n",
    "\n",
    "    if task_type in [\"regression\",\"rg\"]:\n",
    "        model.add(Dense(1,activation = 'linear'))\n",
    "    elif task_type in [\"classify\",\"classification\"]:\n",
    "        pass\n",
    "    elif task_type in [\"regressionPlus\"]:\n",
    "        model.add(Dense(1,activation = 'sigmoid'))\n",
    "    # if task_type in [\"regression\",\"rg\"]:\n",
    "    #     model.compile(loss='mse',optimizer = 'adam')\n",
    "    return model\n",
    "\n",
    "def MakeModel_Nlayers():\n",
    "    pass\n",
    "def MakeModel_Nnodes():\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ Define the model @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "model01 = make_NN(X_train,y_train)\n",
    "train_pred = NN_Predict(model01,X_train)\n",
    "train_pred = train_pred.flatten()\n",
    "train_pred = train_pred * lot_ACV_train\n",
    "valid_pred = NN_Predict(model01,X_valid)\n",
    "valid_pred = valid_pred.flatten()\n",
    "valid_pred = valid_pred * lot_ACV_test\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "rsme_train = np.sqrt(mean_squared_error(sale_price_train,train_pred))\n",
    "rsme_valid = np.sqrt(mean_squared_error(sale_price_test,valid_pred))\n",
    "\n",
    "\n",
    "# y_train = y_train.values.tolist()\n",
    "# y_valid = y_valid.values.tolist()\n",
    "y_compare_train = [sale_price_train,train_pred]\n",
    "y_compare_valid = [sale_price_test,valid_pred]\n",
    "for i in range(100):\n",
    "    print(sale_price_train[i],train_pred[i])\n",
    "print(\"*\"*30)\n",
    "for i in range(100):\n",
    "    print(sale_price_test[i],valid_pred[i] )\n",
    "\n",
    "print(rsme_train,rsme_valid)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
